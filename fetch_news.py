import feedparser
import json
import time
import google.generativeai as genai
from datetime import datetime
import os
from dotenv import load_dotenv

load_dotenv()

#Gemini APIã‚­ãƒ¼ã‚’è¨­å®š
GEMINI_API_KEY = os.environ.get('GEMINI_API_KEY')
if not GEMINI_API_KEY:
    raise ValueError("âŒ GEMINI_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚.envãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯ç’°å¢ƒå¤‰æ•°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

genai.configure(api_key=GEMINI_API_KEY)
print(f"âœ… APIã‚­ãƒ¼ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {GEMINI_API_KEY[:10]}...")

RSS_FEEDS = {
    'technology': [
        'https://news.yahoo.co.jp/rss/topics/it.xml',
        'https://www.itmedia.co.jp/rss/2.0/news_bursts.xml',
        'https://forest.watch.impress.co.jp/data/rss/1.0/wf/feed.rdf'
    ],
    'business': [
        'https://news.yahoo.co.jp/rss/topics/business.xml',
        'https://www.nhk.or.jp/rss/news/cat0.xml',
        'https://biz-journal.jp/index.xml'
    ],
    'sports': [
        'https://news.yahoo.co.jp/rss/topics/sports.xml',
        'https://www.nhk.or.jp/rss/news/cat6.xml',
        'https://www.nikkansports.com/rss/news.rdf'
    ],
    'entertainment': [
        'https://news.yahoo.co.jp/rss/topics/entertainment.xml',
        'https://natalie.mu/music/feed/news',
        'https://www.cinematoday.jp/rss/1.0/news.rdf'
    ]
}

def fetch_rss_feed(url, max_items=5):
    """RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰è¨˜äº‹ã‚’å–å¾—"""
    try:
        print(f"ğŸ“¡ Fetching: {url}")
        feed = feedparser.parse(url)
        return feed.entries[:max_items]
    except Exception as e:
        print(f"âŒ Error fetching {url}: {str(e)}")
        return []

def clean_text(text):
    """HTMLã‚¿ã‚°ã‚’é™¤å»"""
    if not text:
        return ""
    import re
    text = re.sub(r'<[^>]+>', '', text)
    text = text.replace('&nbsp;', ' ').replace('&amp;', '&')
    return text.strip()

def generate_tags_with_gemini(title, description):
    """Gemini APIã§ã‚¿ã‚°ã‚’ç”Ÿæˆ"""
    try:
        model = genai.GenerativeModel('gemini-robotics-er-1.5-preview')
        
        prompt = f"""ä»¥ä¸‹ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã«å¯¾ã—ã¦ã€å†…å®¹ã‚’è¡¨ã™3-5å€‹ã®é©åˆ‡ãªæ—¥æœ¬èªã‚¿ã‚°ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
ã‚¿ã‚°ã¯ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

ã‚¿ã‚¤ãƒˆãƒ«: {title}
æ¦‚è¦: {description[:200]}

ã‚¿ã‚°ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼ˆèª¬æ˜æ–‡ã¯ä¸è¦ï¼‰:"""

        response = model.generate_content(prompt)
        tags_text = response.text.strip()
        
        tags = [tag.strip() for tag in tags_text.replace('ã€', ',').split(',')]
        tags = [tag for tag in tags if tag and len(tag) < 20][:5]
        
        print(f"  ğŸ·ï¸  Tags: {', '.join(tags)}")
        return tags
        
    except Exception as e:
        print(f"  âŒ Gemini API error: {str(e)}")
        return ['AIåˆ†æä¸­']

def format_date(date_str):
    """æ—¥ä»˜ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    try:
        if hasattr(date_str, 'tm_year'):
            return time.strftime('%Y-%m-%d %H:%M:%S', date_str)
        return date_str
    except:
        return datetime.now().strftime('%Y-%m-%d %H:%M:%S')

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print('ğŸš€ Starting news aggregation with Gemini API...\n')
    
    all_articles = []
    article_id = 1
    
    for category, feeds in RSS_FEEDS.items():
        print(f'\nğŸ“‚ Category: {category.upper()}')
        
        for feed_url in feeds:
            entries = fetch_rss_feed(feed_url, max_items=4)
            
            for entry in entries:
                title = clean_text(entry.get('title', ''))
                description = clean_text(entry.get('description', '') or entry.get('summary', ''))[:300]
                link = entry.get('link', '')
                pub_date = format_date(entry.get('published_parsed', ''))
                
                if title and link:
                    print(f"\n  ğŸ“° Processing: {title[:60]}...")
                    
                    #Gemini APIã§ã‚¿ã‚°ã‚’ç”Ÿæˆ
                    tags = generate_tags_with_gemini(title, description)
                    
                    all_articles.append({
                        'id': article_id,
                        'title': title,
                        'link': link,
                        'description': description,
                        'category': category,
                        'tags': tags,
                        'pubDate': pub_date,
                        'source': feed_url.split('/')[2].replace('www.', '')
                    })
                    
                    article_id += 1
                    
                    time.sleep(1)
    
    # all_topics.jsonã«ä¿å­˜
    print(f'\nğŸ’¾ Saving {len(all_articles)} articles to all_topics.json...')
    with open('all_topics.json', 'w', encoding='utf-8') as f:
        json.dump(all_articles, f, ensure_ascii=False, indent=2)
    
    print('âœ… Done! News aggregation completed successfully.')
    print(f'ğŸ“Š Total articles: {len(all_articles)}')
    
    # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®çµ±è¨ˆ
    for category in RSS_FEEDS.keys():
        count = sum(1 for a in all_articles if a['category'] == category)
        print(f'   - {category}: {count} articles')

if __name__ == '__main__':
    main()